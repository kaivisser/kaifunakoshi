```ruby
import matplotlib.pyplot as plt
import statsmodels.tsa.seasonal as smt
import numpy as np 
import pandas as pd 
import random
import datetime as dt
from collections import deque
from sklearn import linear_model 
from sklearn import preprocessing
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import scale
import tensorflow as tf

#import data
DATA = pd.read_csv('DATA/dataset.csv')
DATA = DATA.rename(columns={'Unnamed: 0': 'time'})

df = DATA.drop(['AAPL','AAPL.1','AAPL.2',
                'AMZN','AMZN.1','AMZN.2',
                'GOOGL','GOOGL.1','GOOGL.2',
                'INTC','INTC.1','INTC.2',
                'MSFT','MSFT.1','MSFT.2',
                'NFLX','NFLX.1','NFLX.2'], axis=1) #3 = price, 4 = volume
main_df = df.drop(df.index[[0,1]])
for c in main_df.columns:
    print(c)

#dataframe manipulation & model parameters
SEQ = 60
FUTURES = 5 #future period to predict (minutes)
STOCK = 'AAPL' #stock to predict [AAPL, AMZN, GOOGL, INTC, MSFT, NFLX]
EPOCHS = 10
BATCH_SIZE = 128
NAME = f'{SEQ}-SEQ-{STOCK}'

def classify(current, future): #classify the change (1 = price increase)
    if float(future) > float(current):
        return 1
    else:
        return 0

main_df['future'] = main_df[f'{STOCK}.3'].shift(-FUTURES)
main_df['target'] = list(map(classify, main_df[f'{STOCK}.3'], main_df['future']))

print(main_df[[f'{STOCK}.3','future','target']].head(10))

#prepare & balance train / test data
times = sorted(main_df.time.values)
last5 = times[-int(0.05*len(times))]

scaler = MinMaxScaler(-1,1)   

def preprocess_df(df):
    df = df.drop("future", 1)
    for col in df.columns:
        if col != "target":  
            #if col != 'time':
            df[col] = df[col].astype(float).pct_change()
            df[col] = scale(df[col].values)  

    df.dropna(inplace=True)
    
    sequential_data = []
    prev_days = deque(maxlen=SEQ)
    
    for i in df.values:
        prev_days.append([n for n in i[:-1]])
        if len(prev_days) == SEQ:
            sequential_data.append([np.array(prev_days), i[-1]])
            
    random.shuffle(sequential_data)
    
    #balance data
    
    buys = [] 
    sells = []
    
    for seq, target in sequential_data:
        if target == 0:
            sells.append([seq, target])
        elif target == 1:
            buys.append([seq, target])
    
    random.shuffle(buys)
    random.shuffle(sells)
    
    lower = min(len(buys), len(sells))
    
    buys = buys[:lower]
    sells = sells[:lower]
    
    sequential_data = buys + sells
    random.shuffle(sequential_data)
    
    X = []
    Y = []
    
    for seq, target in sequential_data:
        X.append(seq)
        Y.append(target)
    df.dropna(inplace=True)
    
    return np.array(X), Y  

test = main_df[(main_df.time >= last5)]
train = main_df[(main_df.time <= last5)]
test_data = test.drop('time', axis = 'columns')
train_data = train.drop('time', axis = 'columns')

train_x, train_y = preprocess_df(train_data)
test_x, test_y = preprocess_df(test_data)

print(len(train_x), len(train_y), len(test_x), len(test_y))

#build model

model = tf.keras.models.Sequential()

model.add(tf.keras.layers.CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences = True))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.CuDNNLSTM(128, return_sequences = True))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.CuDNNLSTM(128))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Dense(32, activation = 'relu'))
model.add(tf.keras.layers.Dropout(0.2))

model.add(tf.keras.layers.Dense(2, activation = 'softmax'))

model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.001, decay = 1e-6),
             loss = 'sparse_categorical_crossentropy',
             metrics = ['accuracy'])

history = model.fit(
    train_x, train_y,
    batch_size = BATCH_SIZE, 
    epochs = EPOCHS,
    validation_data = (test_x, test_y))
```
